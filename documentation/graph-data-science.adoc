= Graph Algorithms
:icons: font

== Neo4j Graph Data Science

The Neo4j Graph Data Science (GDS) library contains a set of graph algorithms, exposed through Cypher procedures.
Graph algorithms provide insights into the graph structure and elements, for example, by computing centrality and similarity scores, and detecting communities.
The GDS library is divided into three tiers of maturity: product, beta and alpha.

This guide follows the ordinary workflow for running the product tier algorithms: PageRank, Label Propagation, Weakly Connected Components, Louvain, and Node Similarity.

* Estimate memory usage for your graph and the algorithm you want to run.
* Create a graph and manage created graphs.
* Configure the algorithm to suit your needs and run it in one of the supported modes: stream, write, and stats.

For more resources, see link:https://neo4j.com/developer/graph-data-science/[the developer guides^].

The official Graph Data Science (GDS) library documentation can be found link:https://neo4j.com/docs/graph-data-science/current/[here^].


== The example dataset

image::https://upload.wikimedia.org/wikipedia/en/2/24/AStormOfSwords.jpg[float="right",width=150]

Before you can run any of the algorithms, you need to import your data in Neo4j. +
The example dataset used to demonstrate the GDS library is based on the Game of Thrones fantasy saga.
You may recognize it from the blogs, events, and sandbox.
However, both data and queries are different enough from previous installments that it merits your attention.
{nbsp} +
{nbsp} +
{nbsp} +

=== Attribution

The dataset is partly based on the following works:

_https://networkofthrones.wordpress.com/[Network of Thrones, A Song of Math and Westeros^], research by Dr. Andrew Beveridge._ +
_https://www.macalester.edu/~abeverid/index.html[A. Beveridge and J. Shan, "Network of Thrones," Math Horizons Magazine , Vol. 23, No. 4 (2016), pp. 18-22^]_ +
_https://www.kaggle.com/mylesoneill/game-of-thrones[Game of Thrones, Explore deaths and battles from this fantasy world], by Myles O'Neill, https://www.kaggle.com/[https://www.kaggle.com/^]_ +
_https://github.com/tomasonjo/neo4j-game-of-thrones[Game of Thrones^], by Tomaz Bratanic, GitHub repository._

== Graph of character interactions.. and more

The graph contains `:Person` nodes, representing the characters, and `:INTERACTS` relationships, representing the characters' interactions.
An interaction occurs each time two characters' names (or nicknames) *appear within 15 words of one another* in the book text.
For more information about the data extraction process, see _https://networkofthrones.wordpress.com/from-book-to-network/[Network of Thrones, A Song of Math and Westeros^], research by Dr. Andrew Beveridge._

The `(:Person)-[:INTERACTS]->(:Person)` graph is enriched with data on houses, battles, commanders, kings, knights, regions, locations, and deaths.

Now, let's import the data.


== Data ingestion

.Enable `multi statement queries`
[source]
----
:config "enableMultiStatementMode":true
----

.Create unique constraints on the names of the nodes `:Location`, `:Region`, `:Battle`, `:Person`, and `:House`. This ensures your data integrity and improves performance.
[source, cypher]
----
CREATE CONSTRAINT ON (n:Location) ASSERT n.name IS UNIQUE;
CREATE CONSTRAINT ON (n:Region) ASSERT n.name IS UNIQUE;
CREATE CONSTRAINT ON (n:Battle) ASSERT n.name IS UNIQUE;
CREATE CONSTRAINT ON (n:Person) ASSERT n.name IS UNIQUE;
CREATE CONSTRAINT ON (n:House) ASSERT n.name IS UNIQUE;
----

.Then, ingest the data.
[source, cypher]
----
LOAD CSV WITH HEADERS FROM 'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/battles.csv' AS row
MERGE (b:Battle {name: row.name})
  ON CREATE SET b.year = toInteger(row.year),
  b.summer = row.summer,
  b.major_death = row.major_death,
  b.major_capture = row.major_capture,
  b.note = row.note,
  b.battle_type = row.battle_type,
  b.attacker_size = toInteger(row.attacker_size),
  b.defender_size = toInteger(row.defender_size);

LOAD CSV WITH HEADERS FROM 'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/battles.csv' AS row

// Because there is only attacker_outcome in the data, do a CASE statement for defender_outcome.
WITH row,
     CASE WHEN row.attacker_outcome = 'win' THEN 'loss'
       ELSE 'win'
       END AS defender_outcome

// Match the battle
MATCH (b:Battle {name: row.name})

// All battles have at least one attacker, so you don't have to use FOREACH.
MERGE (attacker1:House {name: row.attacker_1})
MERGE (attacker1)-[a1:ATTACKER]->(b)
  ON CREATE SET a1.outcome = row.attacker_outcome

// Use FOREACH to skip the null values.
FOREACH
(ignoreMe IN CASE WHEN row.defender_1 IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (defender1:House {name: row.defender_1})
  MERGE (defender1)-[d1:DEFENDER]->(b)
    ON CREATE SET d1.outcome = defender_outcome
)
FOREACH
(ignoreMe IN CASE WHEN row.defender_2 IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (defender2:House {name: row.defender_2})
  MERGE (defender2)-[d2:DEFENDER]->(b)
    ON CREATE SET d2.outcome = defender_outcome
)
FOREACH
(ignoreMe IN CASE WHEN row.attacker_2 IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (attacker2:House {name: row.attacker_2})
  MERGE (attacker2)-[a2:ATTACKER]->(b)
    ON CREATE SET a2.outcome = row.attacker_outcome
)
FOREACH
(ignoreMe IN CASE WHEN row.attacker_3 IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (attacker2:House {name: row.attacker_3})
  MERGE (attacker3)-[a3:ATTACKER]->(b)
    ON CREATE SET a3.outcome = row.attacker_outcome
)
FOREACH
(ignoreMe IN CASE WHEN row.attacker_4 IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (attacker4:House {name: row.attacker_4})
  MERGE (attacker4)-[a4:ATTACKER]->(b)
    ON CREATE SET a4.outcome = row.attacker_outcome
);

LOAD CSV WITH HEADERS FROM
'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/battles.csv'
AS row
MATCH (b:Battle {name: row.name})

// Use coalesce to replace the null values with "Unknown".
MERGE (location:Location {name: coalesce(row.location, 'Unknown')})
MERGE (b)-[:IS_IN]->(location)
MERGE (region:Region {name: row.region})
MERGE (location)-[:IS_IN]->(region);

LOAD CSV WITH HEADERS FROM 'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/battles.csv' AS row

// Split the columns that may contain more than one person.
WITH row,
     split(row.attacker_commander, ',') AS att_commanders,
     split(row.defender_commander, ',') AS def_commanders,
     split(row.attacker_king, '/') AS att_kings,
     split(row.defender_king, '/') AS def_kings,
     row.attacker_outcome AS att_outcome,
     CASE WHEN row.attacker_outcome = 'win' THEN 'loss'
       ELSE 'win'
       END AS def_outcome
MATCH (b:Battle {name: row.name})

UNWIND att_commanders AS att_commander
MERGE (p:Person {name: trim(att_commander)})
MERGE (p)-[ac:ATTACKER_COMMANDER]->(b)
  ON CREATE SET ac.outcome = att_outcome

// To end the unwind and correct cardinality(number of rows), use any aggregation function ( e.g. count(*)).
WITH b, def_commanders, def_kings, att_kings, att_outcome, def_outcome,
     COUNT(*) AS c1
UNWIND def_commanders AS def_commander
MERGE (p:Person {name: trim(def_commander)})
MERGE (p)-[dc:DEFENDER_COMMANDER]->(b)
  ON CREATE SET dc.outcome = def_outcome

// Reset cardinality with an aggregation function (end the unwind).
WITH b, def_kings, att_kings, att_outcome, def_outcome, COUNT(*) AS c2
UNWIND def_kings AS def_king
MERGE (p:Person {name: trim(def_king)})
MERGE (p)-[dk:DEFENDER_KING]->(b)
  ON CREATE SET dk.outcome = def_outcome

// Reset cardinality with an aggregation function (end the unwind).
WITH b, att_kings, att_outcome, COUNT(*) AS c3
UNWIND att_kings AS att_king
MERGE (p:Person {name: trim(att_king)})
MERGE (p)-[ak:ATTACKER_KING]->(b)
  ON CREATE SET ak.outcome = att_outcome;

LOAD CSV WITH HEADERS FROM
'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/character-deaths.csv'
AS row

WITH row,
     CASE WHEN row.Nobility = '1' THEN 'Noble'
       ELSE 'Commoner'
       END AS status_value

// Remove House for better linking.
MERGE (house:House {name: replace(row.Allegiances, 'House ', '')})
MERGE (person:Person {name: row.Name})

SET person.gender = CASE WHEN row.Gender = '1' THEN 'male'
  ELSE 'female'
  END,
person.book_intro_chapter = row.`Book Intro Chapter`,
person.book_death_chapter = row.`Death Chapter`,
person.book_of_death = row.`Book of Death`,
person.death_year = toInteger(row.`Death Year`)
MERGE (person)-[:BELONGS_TO]->(house)
MERGE (status:Status {name: status_value})
MERGE (person)-[:HAS_STATUS]->(status)

// Use FOREACH to skip the null values.
FOREACH
(ignoreMe IN CASE WHEN row.GoT = '1' THEN [1]
  ELSE []
  END |
  MERGE (book1:Book {sequence: 1})
    ON CREATE SET book1.name = 'Game of thrones'
  MERGE (person)-[:APPEARED_IN]->(book1)
)
FOREACH
(ignoreMe IN CASE WHEN row.CoK = '1' THEN [1]
  ELSE []
  END |
  MERGE (book2:Book {sequence: 2})
    ON CREATE SET book2.name = 'Clash of kings'
  MERGE (person)-[:APPEARED_IN]->(book2)
)
FOREACH
(ignoreMe IN CASE WHEN row.SoS = '1' THEN [1]
  ELSE []
  END |
  MERGE (book3:Book {sequence: 3})
    ON CREATE SET book3.name = 'Storm of swords'
  MERGE (person)-[:APPEARED_IN]->(book3)
)
FOREACH
(ignoreMe IN CASE WHEN row.FfC = '1' THEN [1]
  ELSE []
  END |
  MERGE (book4:Book {sequence: 4})
    ON CREATE SET book4.name = 'Feast for crows'
  MERGE (person)-[:APPEARED_IN]->(book4)
)
FOREACH
(ignoreMe IN CASE WHEN row.DwD = '1' THEN [1]
  ELSE []
  END |
  MERGE (book5:Book {sequence: 5})
    ON CREATE SET book5.name = 'Dance with dragons'
  MERGE (person)-[:APPEARED_IN]->(book5)
)
FOREACH
(ignoreMe IN CASE WHEN row.`Book of Death` IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: toInteger(row.`Book of Death`)})
  MERGE (person)-[:DIED_IN]->(book)
);

LOAD CSV WITH HEADERS FROM
'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/character-predictions.csv'
AS row
MERGE (p:Person {name: row.name})
// Set properties on the person node.
SET p.title = row.title,
p.death_year = toInteger(row.DateoFdeath),
p.birth_year = toInteger(row.dateOfBirth),
p.age = toInteger(row.age),
p.gender = CASE WHEN row.male = '1' THEN 'male'
  ELSE 'female'
  END

// Use FOREACH to skip the null values.
FOREACH
(ignoreMe IN CASE WHEN row.mother IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (mother:Person {name: row.mother})
  MERGE (p)-[:RELATED_TO {name: 'mother'}]->(mother)
)
FOREACH
(ignoreMe IN CASE WHEN row.spouse IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (spouse:Person {name: row.spouse})
  MERGE (p)-[:RELATED_TO {name: 'spouse'}]->(spouse)
)
FOREACH
(ignoreMe IN CASE WHEN row.father IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (father:Person {name: row.father})
  MERGE (p)-[:RELATED_TO {name: 'father'}]->(father)
)
FOREACH
(ignoreMe IN CASE WHEN row.heir IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (heir:Person {name: row.heir})
  MERGE (p)-[:RELATED_TO {name: 'heir'}]->(heir)
)

// Remove "House " from the value for better linking.
FOREACH
(ignoreMe IN CASE WHEN row.house IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (house:House {name: replace(row.house, 'House ', '')})
  MERGE (p)-[:BELONGS_TO]->(house)
);

LOAD CSV WITH HEADERS FROM
'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/character-predictions.csv'
AS row

MERGE (p:Person {name: row.name})

// Use FOREACH to skip the null values. Lower row.culture for better linking.
FOREACH
(ignoreMe IN CASE WHEN row.culture IS NOT NULL THEN [1]
  ELSE []
  END |
  MERGE (culture:Culture {name: toLower(row.culture)})
  MERGE (p)-[:MEMBER_OF_CULTURE]->(culture)
)
FOREACH
(ignoreMe IN CASE WHEN row.book1 = '1' THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: 1})
  MERGE (p)-[:APPEARED_IN]->(book)
)
FOREACH
(ignoreMe IN CASE WHEN row.book2 = '1' THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: 2})
  MERGE (p)-[:APPEARED_IN]->(book)
)
FOREACH
(ignoreMe IN CASE WHEN row.book3 = '1' THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: 3})
  MERGE (p)-[:APPEARED_IN]->(book)
)
FOREACH
(ignoreMe IN CASE WHEN row.book4 = '1' THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: 4})
  MERGE (p)-[:APPEARED_IN]->(book)
)
FOREACH
(ignoreMe IN CASE WHEN row.book5 = '1' THEN [1]
  ELSE []
  END |
  MERGE (book:Book {sequence: 5})
  MERGE (p)-[:APPEARED_IN]->(book)
);

LOAD CSV WITH HEADERS FROM 'https://s3.eu-north-1.amazonaws.com/com.neo4j.gds.browser-guide/data/character-predictions.csv' AS row

WITH row,
     CASE WHEN row.isAlive = '0' THEN [1]
       ELSE []
       END AS dead_person,
     CASE WHEN row.isAliveMother = '0' THEN [1]
       ELSE []
       END AS dead_mother,
     CASE WHEN row.isAliveFather = '0' THEN [1]
       ELSE []
       END AS dead_father,
     CASE WHEN row.isAliveHeir = '0' THEN [1]
       ELSE []
       END AS dead_heir,
     CASE WHEN row.isAliveSpouse = '0' THEN [1]
       ELSE []
       END AS dead_spouse

MATCH (p:Person {name: row.name})

// Use OPTIONAL MATCH (mother:Person {name: row.mother}) not to stop the query if the Person is not found.
OPTIONAL MATCH (mother:Person {name: row.mother})
OPTIONAL MATCH (father:Person {name: row.father})
OPTIONAL MATCH (heir:Person {name: row.heir})
OPTIONAL MATCH (spouse:Spouse {name: row.spouse})

// Set the label Dead to each dead person.
FOREACH (d IN dead_person |
  SET p:Dead
)
FOREACH (d IN dead_mother |
  SET mother:Dead
)
FOREACH (d IN dead_father |
  SET father:Dead
)
FOREACH (d IN dead_heir |
  SET heir:Dead
)
FOREACH (d IN dead_spouse |
  SET spouse:Dead
);

MATCH (p:Person) where exists (p.death_year)
SET p:Dead;

MATCH (p:Person)-[:DEFENDER_KING|ATTACKER_KING]-()
SET p:King;

MATCH (p:Person) where toLower(p.title) contains "king"
SET p:King;

MATCH (p:Person) where p.title = "Ser"
SET p:Knight;

// Map the names coming from the different data sources.
:param [map] => {
  RETURN
    {
      `Aemon Targaryen (Maester Aemon)`: 'Aemon Targaryen (son of Maekar I)',
      `Arstan`:                          'Barristan Selmy',
      `Garin (orphan)`:                  'Garin (Orphan)',
      `Hareth (Moles Town)`:             "Hareth (Mole's Town)",
      `Jaqen Hghar`:                     "Jaqen H'ghar",
      `Lommy Greenhands`:                'Lommy',
      `Rattleshirt`:                     'Lord of Bones',
      `Thoros of Myr`:                   'Thoros'
    } AS map
};

LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/mathbeveridge/asoiaf/2d8ded13eda5128ace5e3b995253d69e62bc4bf6/data/asoiaf-book1-edges.csv' AS row
WITH replace(row.Source, '-', ' ') AS srcName,
     replace(row.Target, '-', ' ') AS tgtName,
     toInteger(row.weight) AS weight
MERGE (src:Person {name: coalesce($map[srcName], srcName)})
MERGE (tgt:Person {name: coalesce($map[tgtName], tgtName)})
MERGE (src)-[i:INTERACTS {book: 1}]->(tgt)
  ON CREATE SET i.weight = weight
  ON MATCH SET i.weight = i.weight + weight
MERGE (src)-[r:INTERACTS_1]->(tgt)
  ON CREATE SET r.weight = weight, r.book = 1;

LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/mathbeveridge/asoiaf/2d8ded13eda5128ace5e3b995253d69e62bc4bf6/data/asoiaf-book2-edges.csv' AS row
WITH replace(row.Source, '-', ' ') AS srcName,
     replace(row.Target, '-', ' ') AS tgtName,
     toInteger(row.weight) AS weight
MERGE (src:Person {name: coalesce($map[srcName], srcName)})
MERGE (tgt:Person {name: coalesce($map[tgtName], tgtName)})
MERGE (src)-[i:INTERACTS {book: 2}]->(tgt)
  ON CREATE SET i.weight = weight
  ON MATCH SET i.weight = i.weight + weight
MERGE (src)-[r:INTERACTS_2]->(tgt)
  ON CREATE SET r.weight = weight, r.book = 2;

LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/mathbeveridge/asoiaf/2d8ded13eda5128ace5e3b995253d69e62bc4bf6/data/asoiaf-book3-edges.csv' AS row
WITH replace(row.Source, '-', ' ') AS srcName,
     replace(row.Target, '-', ' ') AS tgtName,
     toInteger(row.weight) AS weight
MERGE (src:Person {name: coalesce($map[srcName], srcName)})
MERGE (tgt:Person {name: coalesce($map[tgtName], tgtName)})
MERGE (src)-[i:INTERACTS {book: 3}]->(tgt)
  ON CREATE SET i.weight = weight
  ON MATCH SET i.weight = i.weight + weight
MERGE (src)-[r:INTERACTS_3]->(tgt)
  ON CREATE SET r.weight = weight, r.book = 3;

LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/mathbeveridge/asoiaf/2d8ded13eda5128ace5e3b995253d69e62bc4bf6/data/asoiaf-book4-edges.csv' AS row
WITH replace(row.Source, '-', ' ') AS srcName,
     replace(row.Target, '-', ' ') AS tgtName,
     toInteger(row.weight) AS weight
MERGE (src:Person {name: coalesce($map[srcName], srcName)})
MERGE (tgt:Person {name: coalesce($map[tgtName], tgtName)})
MERGE (src)-[i:INTERACTS {book: 4}]->(tgt)
  ON CREATE SET i.weight = weight
  ON MATCH SET i.weight = i.weight + weight
MERGE (src)-[r:INTERACTS_4]->(tgt)
  ON CREATE SET r.weight = weight, r.book = 4;

LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/mathbeveridge/asoiaf/2d8ded13eda5128ace5e3b995253d69e62bc4bf6/data/asoiaf-book5-edges.csv' AS row
WITH replace(row.Source, '-', ' ') AS srcName,
     replace(row.Target, '-', ' ') AS tgtName,
     toInteger(row.weight) AS weight
MERGE (src:Person {name: coalesce($map[srcName], srcName)})
MERGE (tgt:Person {name: coalesce($map[tgtName], tgtName)})
MERGE (src)-[i:INTERACTS {book: 5}]->(tgt)
  ON CREATE SET i.weight = weight
  ON MATCH SET i.weight = i.weight + weight
MERGE (src)-[r:INTERACTS_5]->(tgt)
  ON CREATE SET r.weight = weight, r.book = 5;
----


== Data visualization

Let's briefly explore the dataset before running some algorithms.

Run the following query to visualize the schema of your graph:

[source,cypher]
----
CALL db.schema.visualization()
----

The `:Dead`, `:King`, and `:Knight` labels all appear on `:Person` nodes.
You may find it useful to remove them from the visualization to make it easier to inspect.

== Summary statistics

Calculate some simple statistics to see how data is distributed.
For example, find the minimum, maximum, average, and standard deviation of the number of interactions per character:

[source,cypher]
----
MATCH (c:Person)-[:INTERACTS]->()
WITH c, count(*) AS num
RETURN min(num) AS min, max(num) AS max, avg(num) AS avg_interactions, stdev(num) AS stdev
----

Calculate the same grouped by book:

[source,cypher]
----
MATCH (c:Person)-[r:INTERACTS]->()
WITH r.book AS book, c, count(*) AS num
RETURN book, min(num) AS min, max(num) AS max, avg(num) AS avg_interactions, stdev(num) AS stdev
ORDER BY book
----


== Estimate memory usage: why?

Now that you have data and know something about its shape, you need to estimate the memory usage of your graph and algorithm(s), and to configure your Neo4j Server with a much larger heap size than for a transactional deployment.
Why?

Because, the graph algorithms run on an in-memory, heap-allocated projection of the Neo4j graph, which resides outside the main database.
This means that before you execute an algorithm, you must create (explicitly or implicitly) a projection of your graph in memory.

However, creating graphs and running algorithms on them can have a significant memory footprint.

Therefore, a good habit is always to estimate the amount of RAM you need and configure a large heap size before running a heavy memory workload.

In the following three chapters, you will be able to exercise memory estimation and explore its results.

== Memory estimation: graphs

The GDS library offers a set of procedures that can help you estimate the memory needed to create a graph and run algorithms.

To estimate the required memory for a subset of your graph, for example, the `Person` nodes and `INTERACTS` relationships, call the following procedure.

[source, cypher]
----
CALL gds.graph.create.estimate('Person', 'INTERACTS') YIELD nodeCount, relationshipCount, requiredMemory
----

The result shows that the example graph is small.
So, you can create your projected graph and name it, for example, `got-interactions`.

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', 'INTERACTS')
----

== Estimate memory usage: algorithms

To estimate the memory needed to execute an algorithm on your `got-interactions` graph, for example, Page Rank, call the following procedure.

[source, cypher]
----
CALL gds.pageRank.stream.estimate('got-interactions') YIELD requiredMemory
----

This estimation considers only the algorithm execution, as the graph is already in-memory.

== Estimate memory usage: details

If you want to look at the full details of the memory estimation, remove the `YIELD` clause.
The procedure returns a tree view and a map view of all the "components" with their memory estimates.

[source, cypher]
----
CALL gds.pageRank.stream.estimate('got-interactions')
----

As you see, the more detailed views contain estimates on the individual compute steps and the result data structures.

You can also estimate the memory usage for graph creation and algorithm execution at the same time by using the so-called _implicit graph creation_.
This way, the configuration for the graph creation is inlined within the algorithm procedure call.

[source, cypher]
----
CALL gds.pageRank.stream.estimate({nodeProjection: 'Person', relationshipProjection: 'INTERACTS'})
----

The result shows an increased memory estimate, explained by the memory consumed by the graph creation.

Now, you can filter the result to the top level components: graph and algorithm.

[source, cypher]
----
CALL gds.pageRank.stream.estimate({
  nodeProjection: 'Person',
  relationshipProjection: 'INTERACTS'
}) YIELD mapView
UNWIND [ x IN mapView.components | [x.name, x.memoryUsage] ] AS component
RETURN component[0] AS name, component[1] AS size
----

For more details, see _link:https://neo4j.com/docs/graph-data-science/current/common-usage/memory-estimation/[the Memory Estimation section in the GDS Manual^]_.

== Memory estimation: cleanup

If you do not want to use the projected graph anymore, a good practice is to release it from the memory.

[source, cypher]
----
CALL gds.graph.drop('got-interactions');
----


== Graph creation

The first stage of execution in GDS is always graph creation, but what does this mean?

To enable fast caching of the graph topology, containing only the relevant nodes, relationships, and weights, the GDS library operates on in-memory graphs that are created as projections of the Neo4j stored graph.

These projections may change the nature of the graph elements by any of the following:

* Subgraphing
* Renaming relationship types or node labels
* Merging several relationship types or node labels
* Altering relationship direction
* Aggregating parallel relationships and their properties
* Deriving relationships from larger patterns

There are two ways of creating graphs – _explicit_ and _implicit_.

== Graph catalog

The typical workflow is to create the projected graph _explicitly_ by giving it a name and storing it in the _graph catalog_.
This allows you to operate on the graph multiple times.

In the _Memory estimation_ chapters, you calculated the memory needed for creating a small graph of interactions, called `got-interactions`.
If you have removed it from the memory, you can create it again.
Because each `INTERACTS` relationship is symmetric, you can even ignore its direction by creating your graph with an `UNDIRECTED` orientation.

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', {
  INTERACTS: {
    orientation: 'UNDIRECTED'
  }
})
----

== Graph catalog: standard creation and Cypher projection

The GDS library supports two approaches for loading projected graphs - *standard creation* (`gds.graph.create()`) and *Cypher projection* (`gds.graph.create.cypher()`).

In the *standard creation* approach, which you used to create your graph, you specify node labels and relationship types and project them onto the in-memory graph as labels and relationship types with new names.
You can further specify properties for each node label and relationship type.
For some use cases, this approach might be sufficient.
However, it is not possible to take only some nodes with a given label or only some relationships of a given type.
One way to work around it is by adding additional labels that define the desired subset of nodes that you want to project.

In the *Cypher projection* approach, you use Cypher queries to project nodes and relationships onto the in-memory graph.
Instead of specifying labels and relationship types, you define node-statements and relationship-statements.
In this way, you can leverage the expressivity of the Cypher language and describe your graph in a more sophisticated way.

It is important to note that the standard creation is orders of magnitude faster than the Cypher projection.
When designing a use case with Cypher projection at a production scale, make sure to measure the performance in advance.

Now, let’s try the Cypher projection and load the same graph with a new name, for example, `got-interactions-cypher`.

== Graph catalog: Cypher projection

You specify two queries: one for the nodes and one for the relationships.
You need to return `id`, `source`, and `target` columns and can optionally return label, relationship type and property columns.

[source, cypher]
----
CALL gds.graph.create.cypher(
  'got-interactions-cypher',
  'MATCH (n:Person) RETURN id(n) AS id',
  'MATCH (s:Person)-[i:INTERACTS]->(t:Person) RETURN id(s) AS source, id(t) AS target, i.weight AS weight'
)
----

The first query returns the node IDs; the second one returns the source and target IDs of the relationships, as well as one relationship property `weight`.
Here, you can use any pair of Cypher queries as long as they return the expected columns and field types. +
To aggregate relationships, standard Cypher features can be used, such as `DISTINCT`.
You can find more details about relationship aggregations _link:https://neo4j.com/docs/graph-data-science/current/management-ops/cypher-projection/#cypher-projection-relationship-aggregation[here^]_.

== Graph catalog: Cypher projection of virtual relationships

Another interesting feature of the Cypher graph projection is that it allows you to represent complex patterns by computing relationships that do not exist in the Neo4j stored graph.
This is especially useful when the algorithm you want to run supports only mono-partite graphs. +
For example, you can use the following query to create a graph with `Person` nodes connected with an (untyped) relationship if they belong to the same house.
The projected relationship does not exist in the stored graph.

[source, cypher]
----
CALL gds.graph.create.cypher(
  'same-house-graph',
  'MATCH (n:Person) RETURN id(n) AS id',
  'MATCH (p1:Person)-[:BELONGS_TO]-(:House)-[:BELONGS_TO]-(p2:Person) RETURN id(p1) AS source, id(p2) AS target'
)
----

== Graph catalog: listing

After you create your projected graph, you can try several useful queries to manage it.

You can list all information about it by using following procedure:

[source, cypher]
----
CALL gds.graph.list('got-interactions-cypher')
----

You can list the graphs you have loaded so far by using following procedure:

[source, cypher]
----
CALL gds.graph.list()
----

== Graph catalog: existence

You can check if a graph exists by using the following procedure:

[source, cypher]
----
CALL gds.graph.exists('got-interactions')
----

== Graph catalog: removal

You can free up memory space by dropping some of the created graphs from the catalog:

[source, cypher]
----
CALL gds.graph.drop('got-interactions-cypher');
----

*TIP:* It is a good practice to remove the unused graphs, yours and of the previous users, from the memory.

*NOTE:* Multiple users running algorithms at the same time is not supported.

Now you are ready to run some actual algorithms.

==  Getting started with algorithms

With Neo4j, you can run algorithms on explicitly and implicitly created graphs. +
In this tutorial, we will show you how to get the most out of the following algorithms:

* Page Rank
* Label Propagation
* Weakly Connected Components (WCC)
* Louvain
* Node Similarity
* Triangle Count
* Local Clustering Coefficient


== Algorithm syntax: explicit graphs

Running algorithms on explicitly created graphs allows you to operate on a graph multiple times.
To do this, refer to the graph by its name,  as it is stored in the graph catalog.

[source]
----
CALL gds.<algo-name>.<mode>(
  graphName: String,
  configuration: Map
)
----

* `<algo-name>` is the algorithm name.
* `<mode>` is the algorithm execution mode.
The supported modes are:
** `write`: writes results to the Neo4j database and returns a summary of the results.
** `stats`: same as `write` but does not write to the Neo4j database.
** `stream`: streams results back to the user.
* The `graphName` parameter value is the name of the graph from the graph catalog.
* The `configuration` parameter value is the algorithm-specific configuration.

== Algorithm syntax: implicit graphs

The implicit variant does not access the graph catalog.
If you want to run an algorithm on such a graph, you configure the graph creation within the algorithm configuration map.

[source]
----
CALL gds.<algo-name>.<mode>(
  configuration: Map
)
----

After the algorithm execution finishes, the graph is released from the memory.


== Page Rank

image::https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/PageRanks-Example.svg/758px-PageRanks-Example.svg.png[float="right", width="300"]

Page Rank is an algorithm that measures the transitive influence and connectivity of nodes to find the most *influential* nodes in a graph. +
It computes an influence value for each node, called a _score_.
As a result, the score of a node is a certain weighted average of the scores of its direct neighbors.

*How Page Rank works*

PageRank is an _iterative_ algorithm.
In each iteration, every node propagates its score evenly divided to its neighbors. +
The algorithm runs for a configurable maximum number of iterations (default is 20), or until the node scores converge.
That is, when the maximum change in node score between two sequential iterations is smaller than the configured `tolerance` value.

In the following chapters, you will see how Page Rank identifies the most important nodes.

== Page Rank: stream mode

Let's find out who is influential in the graph by running Page Rank.
If you have removed it from the catalog, you have to create it again:

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', {
  INTERACTS: {
    orientation: 'UNDIRECTED'
  }
})
----

First, you run a basic Page Rank call in `stream` mode.

[source, cypher]
----
CALL gds.pageRank.stream('got-interactions') YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC LIMIT 10
----

Then, you compare the Page Rank of each `Person` node with the number of interactions for that node.

[source,cypher]
----
CALL gds.pageRank.stream('got-interactions') YIELD nodeId, score AS pageRank
WITH gds.util.asNode(nodeId) AS n, pageRank
MATCH (n)-[i:INTERACTS]-()
RETURN n.name AS name, pageRank, count(i) AS interactions
ORDER BY pageRank DESC LIMIT 10
----

The result shows that not always the most talkative characters have the highest rank.

== Page Rank: write mode

Now that you have the results from your Page Rank query, you write them back to Neo4j and use them for further queries. +
You specify the name of the property to which the algorithm will write using the `writeProperty` key in the config map passed to the procedure.

Note that the writing is done in Neo4j, not in the graph `got-interactions`.

[source, cypher]
----
CALL gds.pageRank.write('got-interactions', {writeProperty: 'pageRank'})
----

== Page Rank: rank per book

Along with the generic `INTERACTS` relationships, you also have `INTERACTS_1`, `INTERACTS_2`, etc., for the different books.
Let's load a graph for the interactions in book 1 and compute and write the Page Rank scores.

[source, cypher]
----
CALL gds.graph.create(
  'got-interactions-1',
  'Person',
  {
    INTERACTS_1: {
      orientation: 'UNDIRECTED'
    }
  }
);
----

[source, cypher]
----
CALL gds.pageRank.write(
  'got-interactions-1',
  {
    writeProperty: 'pageRank-1'
  }
)
----

It is generally a good idea to explicitly create the graph before executing an algorithm.
However, if you do not think that you will operate on this graph in the future, you can load it implicitly as part of the algorithm execution.

[source, cypher]
----
CALL gds.pageRank.write({
  nodeProjection: 'Person',
  relationshipProjection: {
    INTERACTS_1: {
      orientation: 'UNDIRECTED'
    }
  },
  writeProperty: 'pageRank-1'
})
----

== Page Rank: exercise

Let's see what you have learned so far.

Try to calculate the Page Rank of the other books in the series and store the results in the database.

* Write queries that call `gds.pageRank.write` for the `INTERACTS_2`, `INTERACTS_3`, `INTERACTS_4`, and `INTERACTS_5` relationship types.
  You can load a graph for each relationship type explicitly, or use the shorthand.

Then, try to write queries to answer the following questions:

* Which character has the biggest increase in influence from book 1 to 5?
* Which character has the biggest decrease?

*Bonus task*

* Use a Cypher projection to create a graph of ``House``s that fought in the same ``Battle``s and run Page Rank.
* Does the result change if you weight Page Rank with the number of shared ``Battle``s?

You can find the solution on the next slide.

== Page Rank: exercise answer

[source, cypher]
----
CALL gds.graph.create.cypher(
  'house-battles',
  'MATCH (h:House) RETURN id(h) AS id',
  'MATCH (h1:House)-->(b:Battle)<--(h2:House) RETURN id(h1) AS source, id(h2) AS target, count(b) AS weight'
)
----

[source, cypher]
----
CALL gds.pageRank.stream(
  'house-battles',
  {
    relationshipWeightProperty: 'weight'
  }
)
YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC
----


== Label Propagation

image::https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/20190226091707/label-propagation-graph-algorithm-1.png[float="right",width=300]

Label Propagation (LPA) is a fast algorithm for finding communities in a graph.
It propagates labels throughout the graph and forms communities of nodes based on their influence.

**How Label Propagation works**

LPA is an _iterative_ algorithm.
First, it assigns a unique community label to each node. +
In each iteration, the algorithm changes this label to the most common one among its neighbors.
Densely connected nodes quickly broadcast their labels across the graph. +
At the end of the propagation, only a few labels remain. +
Nodes that have the same community label at convergence are considered from the same community.
The algorithm runs for a configurable maximum number of iterations, or until it converges.

For more details, see _https://neo4j.com/docs/graph-data-science/current/algorithms/label-propagation/[the documentation^]_.

== Label Propagation: example

Let's run Label Propagation to find the five largest communities of people interacting with each other. +
For flexibility, in this example, you can create the graph directly in the algorithm call. +
The weight property on the relationship represents the number of interactions between two people.
In LPA, the weight is used to determine the influence of neighboring nodes when voting on community assignment.

[source, cypher]
----
CALL gds.graph.create(
  'got-interactions-weighted',
  'Person',
  {
    INTERACTS: {
      orientation: 'UNDIRECTED',
      properties: 'weight'
    }
  }
)
----

Let's now run LPA with just one iteration:

[source, cypher]
----
CALL gds.labelPropagation.stream(
  'got-interactions-weighted',
  {
    relationshipWeightProperty: 'weight',
    maxIterations: 1
  }
) YIELD nodeId, communityId
RETURN communityId, count(nodeId) AS size
ORDER BY size DESC
LIMIT 5
----

You can see that the nodes are assigned to initial communities - 2166	nodes to 1476 communities. +
However, the algorithm needs multiple iterations to achieve a stable result.
So, you run the same procedure with two iterations and see how the results change.

[source, cypher]
----
CALL gds.labelPropagation.stream(
  'got-interactions-weighted',
  {
    relationshipWeightProperty: 'weight',
    maxIterations: 2
  }
) YIELD nodeId, communityId
RETURN communityId, count(nodeId) AS size
ORDER BY size DESC
LIMIT 5
----

Usually, label propagation requires more than a few iterations to converge on a stable result.
The number of the required iterations depends on the graph structure -- you should experiment.

== Label Propagation: seeding

Label Propagation can be seeded with an initial community label from a pre-existing node property.
This allows you to compute communities incrementally. +
Let's write the results after the first iteration back to the source graph, under the write property name `community`.

[source, cypher]
----
CALL gds.labelPropagation.write(
  'got-interactions-weighted',
  {
    relationshipWeightProperty: 'weight',
    maxIterations: 1,
    writeProperty: 'community'
  }
)
----

You can now use the `community` property as a seed property for the second iteration.
The results should be the same as the previous run with two iterations. +
Seeding is particularly useful when the source graph grows and you want to compute communities incrementally, without starting again from scratch.
Since 'got-interactions-weighted' does not contain the 'community' property, you must create a new graph that does.

[source, cypher]
----
CALL gds.graph.create(
  'got-interactions-seeded',
  {
    Person: {
      properties: 'community'
    }
  },
  {
    INTERACTS: {
      orientation: 'UNDIRECTED',
      properties: 'weight'
    }
  }
)
----

And then, you can use the `seed` configuration key to specify the property from which you want to seed community IDs.

[source, cypher]
----
CALL gds.labelPropagation.stream(
  'got-interactions-seeded',
  {
    relationshipWeightProperty: 'weight',
    maxIterations: 1,
    seedProperty: 'community'
  }
) YIELD nodeId, communityId
RETURN communityId, count(nodeId) AS size
ORDER BY size DESC
LIMIT 5
----

== Label Propagation: exercise

Now that you understand the basics of LPA, let's experiment a little.

* How many iterations does it take for LPA to converge on a stable number of communities? How many communities do you end up with?

* What happens when you run LPA for 1,000 maxIterations? (_hint: try using YIELD ranIterations_)

* What happens if you run LPA without weights? Do you find the same communities?

* *Bonus task*: What if you use house affiliations as seeds for communities? How would you use Cypher to create the initial seeds? Run the algorithm with the new seeds. Do you find a different set of communities?

== Label Propagation: cleanup

Now that you are done with Label Propagation, you can remove the graphs from the catalog.

[source, cypher]
----
CALL gds.graph.drop('got-interactions-weighted');
CALL gds.graph.drop('got-interactions-seeded');
----


== Weakly Connected Components

image::https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/20190222092528/union-find-graph-algorithm-visualization-3.png[float="right", width="350"]

The Weakly Connected Components algorithm (previously known as Union Find) finds sets of connected nodes in an _undirected_ graph, where each node is reachable from any other node in the same set.
It is called _weakly_ because it relies on the relationship between two nodes regardless of its direction, wherefore the graph is treated as _undirected_. +
This algorithm is useful for identifying disjoint subgraphs, when pre-processing graphs, or for disambiguation purposes.

Let's start with a simple example that shows how to run the algorithm and stream the results.

== Weakly Connected Components: example

You can use the `got-interactions` graph and run the algorithm to compute components.
If you have removed it from the catalog, you have to create it again:

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', {
  INTERACTS: {
    orientation: 'UNDIRECTED'
  }
})
----

[source, cypher]
----
CALL gds.wcc.stream('got-interactions')
YIELD nodeId, componentId
RETURN componentId AS component, count(nodeId) AS size
ORDER BY size DESC
----

The result is one large component containing 795 characters and many isolated characters.

== Weakly Connected Components: connected components

Let's use a Cypher projection to build a new graph named `got-culture-interactions-cypher`.
It will contain people that belong to the same culture.

[source, cypher]
----
CALL gds.graph.create.cypher(
  'got-culture-interactions-cypher',
  'MATCH (n:Person) RETURN id(n) AS id',
  'MATCH (p1:Person)-[:MEMBER_OF_CULTURE]->(c:Culture)<-[:MEMBER_OF_CULTURE]-(p2:Person) RETURN id(p1) AS source, id(p2) AS target'
)
----

Now, run the algorithm to compute components.

[source, cypher]
----
CALL gds.wcc.stream('got-culture-interactions-cypher')
YIELD nodeId, componentId
RETURN componentId AS component, count(nodeId) AS size ORDER BY size DESC
----

The result is components with different sizes.

Reviewing the results, which cultures are represented by the five largest components?

Can you modify the query to write the components back to the database?
Add the property `wcc_partition` to your `:Person` nodes.

== Weakly Connected Components: thresholds

You can also use some additional configuration options:

* `threshold` for connectivity (used along with `relationshipWeightProperty`)
* `seedProperty`

**Threshold**

If the `threshold` option is specified, the `relationshipWeightProperty` option must also be present.
In this case, relationships whose weight is below the given threshold will not be used in the computation.

You will consider a graph with relationships weighted by the number of times a pair of individuals have interacted.

**Note:** You are casting the weight property from the graph as a float because that is what the algorithm expects as an input.

[source, cypher]
----
CALL gds.graph.create('got-wcc-weighted-interactions',
  'Person',
  {
    INTERACTS: {
      orientation: 'NATURAL',
      properties: {
        weight: {
          property: 'weight',
          defaultValue: 0.0,
          aggregation: 'SINGLE'
        }
      }
    }
  }
)
----

[source, cypher]
----
CALL gds.wcc.stream(
  'got-wcc-weighted-interactions',
  {
    relationshipWeightProperty:'weight',
    threshold:5.0
  }
)
YIELD nodeId, componentId
RETURN count(distinct componentId) AS components
----

How does the number of identified communities change when you change the threshold?
What happens to their size?
What value produces the most communities?

== Weakly Connected Components: seeding

Now you can use the `wcc_partition` property to seed the algorithm with an initial community label.
This allows you to compute communities incrementally.

If you have not managed to create the property `wcc_partition`, execute the following query.

[source, cypher]
----
CALL gds.wcc.write(
  'got-culture-interactions-cypher',
  {
    writeProperty: 'wcc_partition'
  }
)
----

Then, you can create a projected graph, called `got-wcc-interactions-seeded` and add the property to your `Person` nodes:

[source, cypher]
----
CALL gds.graph.create(
  'got-wcc-interactions-seeded',
  {
    Person: {
      properties: 'wcc_partition'
    }
  },
  {
    INTERACTS: {
      orientation: 'UNDIRECTED',
      properties: 'weight'
    }
  }
)
----

**Seeding**

For the Weakly Connected Components algorithm, this functionality is most useful when you want to add data to an existing graph.

[source, cypher]
----
MATCH (p:Person)
WITH p.wcc_partition AS community, collect(p) AS members
WITH community, size(members) AS size, members[0] AS someGuy
    ORDER BY size DESC
    LIMIT 6
WITH collect(someGuy) AS someGuys
WITH someGuys, someGuys[0] AS first
MERGE (mats:Person {name: 'Mats'})
MERGE (mats)-[:INTERACTS]->(first)
WITH someGuys, someGuys[1] AS second
MERGE (martin:Person {name: 'Martin'})
MERGE (martin)-[:INTERACTS]->(second)
WITH someGuys, someGuys[2] AS third
MERGE (jonatan:Person {name: 'Jonatan'})
MERGE (jonatan)-[:INTERACTS]->(third)
WITH someGuys, someGuys[3] AS fourth
MERGE (max:Person {name: 'Max'})
MERGE (max)-[:INTERACTS]->(fourth)
WITH someGuys, someGuys[4] AS fifth
MERGE (soren:Person {name: 'Soren'})
MERGE (soren)-[:INTERACTS]->(fifth)
WITH someGuys, someGuys[5] AS sixth
MERGE (paul:Person {name: 'Paul'})
MERGE (paul)-[:INTERACTS]->(fourth)
----

Now let's use the previously labeled `wcc_partition` as a seed, and assign communities to your new nodes:

[source, cypher]
----
CALL gds.wcc.stream(
  'got-wcc-interactions-seeded',
  {
    seedProperty: 'wcc_partition'
  }
)
YIELD nodeId, componentId
RETURN componentId, count(nodeId) AS size
ORDER BY size DESC
----

The number of communities is the same as before, but you have also added the properties to the new nodes.
On a small graph this is trivial, but on a large graph this saves a lot of computational time.

== Weakly Connected Components: exercise

* Can you use a Cypher projection to create a graph that contains at least five communities with more than two members?

* Can you use a Cypher projection with thresholding (you can use Cypher to add a new weight property if you want) to break the graph into multiple properties?
Does increasing your threshold create _more_ or _fewer_ partitions?

* Using the previous exercise, write the partitions to the graph, and then use them as seeds for Union Find on the full graph, using `Person` and `INTERACTS`.
How many communities do you find?
What happened?

== Weakly Connected Components: cleanup

To remove the nodes that have been created during the seeding exercise, run the following query:

[source, cypher]
----
MATCH (p:Person) WHERE p.name IN ['Mats', 'Martin', 'Jonatan', 'Max', 'Soren', 'Paul'] DETACH DELETE p
----

To clean up the in-memory graphs created during the exercises, you can run the following queries.

[source, cypher]
----
CALL gds.graph.drop('got-culture-interactions-cypher');
CALL gds.graph.drop('got-wcc-weighted-interactions');
CALL gds.graph.drop('got-wcc-interactions-seeded');
----


== Louvain

image::https://neo4j.com/docs/graph-algorithms/current/images/louvain-multilevel-graph.svg[float="right", width="400"]

The Louvain algorithm, like Label Propagation and Weakly Connected Components, is a community detection algorithm designed to identify clusters of nodes in a graph.
It applies heuristic modularity to define the community structure by calculating how densely connected the nodes within a community (module) are, versus in a random graph.
Louvain also reveals a hierarchy of communities at different scales, which enables you to zoom in on different levels of granularity and find sub-communities within sub-communities within sub-communities.

*How Louvain works*

Louvain is a _greedy_, _hierarchical clustering_ algorithm.
It repeats the following two steps until it finds a global optimum:

. Assign the nodes to communities, favoring local optimizations of modularity.
. Aggregate the nodes from the same community to form a single node, which inherits all connected relationships.

These two steps are repeated until no further modularity-increasing reassignments of communities are possible.
Because ties are broken arbitrarily, you can get different results between different runs of the Louvain algorithm.

*What to consider*

Louvain is significantly slower than Label Propagation and Weakly Connected Components, and the results can be hard to interpret.

The algorithm is sensitive to the weighting scheme used on the relationships.
A good sign that you need to tweak your schema or weighting is when you notice that the results include only a _single_ giant community, or every node is a community on its own.

== Louvain: examples

Let's compute the Louvain community structure of the graph `got-interactions`.
If you have removed it from the catalog, you have to create it again:

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', {
  INTERACTS: {
    orientation: 'UNDIRECTED'
  }
})
----

[source, cypher]
----
CALL gds.louvain.stream('got-interactions')
YIELD nodeId, communityId
RETURN gds.util.asNode(nodeId).name AS person, communityId
ORDER BY communityId DESC
----

The query returns the name of each person and the id of the community to which it belongs.
If you want to investigate how many communities are available, and the number of members of each community, you can change the RETURN statement.

[source, cypher]
----
CALL gds.louvain.stream('got-interactions')
YIELD nodeId, communityId
RETURN communityId, COUNT(DISTINCT nodeId) AS members
ORDER BY members DESC
----

The result is 1382 communities, 11 of which with more than one member.

== Louvain: weighting

Now let's run the Louvain algorithm on a weighted graph.
This way, it considers the relationship weights when calculating the modularity.

First, you must create a graph with the `weight` relationship property.
Otherwise, the number specified in `defaultValue` will be used as a fallback.

[source, cypher]
----
CALL gds.graph.create(
  'got-weighted-interactions',
  'Person',
  {
    INTERACTS: {
      orientation: 'UNDIRECTED',
      aggregation: 'NONE',
      properties: {
      	weight: {
          property: 'weight',
          aggregation: 'NONE',
          defaultValue: 0.0
        }
      }
    }
  }
)
----

Then, use the `weight` property on the INTERACTS relationship and see what happens:

[source,cypher]
----
CALL gds.louvain.stream(
  'got-weighted-interactions',
  {
    relationshipWeightProperty: 'weight'
  }
)
YIELD nodeId, communityId
RETURN communityId, COUNT(DISTINCT nodeId) AS members
ORDER BY members DESC
----

The result is 1384 communities, 13 of which with more than one member.

== Louvain: intermediate communities

Now let's try to identify communities at multiple levels in the graph: first small communities, and then combine them in large ones.

To retrieve the intermediate communities, set `includeIntermediateCommunities` to `true`:

[source,cypher]
----
CALL gds.louvain.stream(
  'got-interactions',
  {
    includeIntermediateCommunities: true
  }
)
YIELD nodeId, communityId, intermediateCommunityIds
RETURN communityId, COUNT(DISTINCT nodeId) AS members, intermediateCommunityIds
----

You can extract membership in different levels of communities and see how the composition changes:

[source,cypher]
----
CALL gds.louvain.stream(
  'got-interactions',
  {
    includeIntermediateCommunities: true
  }
)
YIELD nodeId, intermediateCommunityIds
RETURN count(distinct intermediateCommunityIds[0]), count(distinct intermediateCommunityIds[1])
----

`includeIntermediateCommunities: false` is the default value, in which case, the `intermediateCommunityIds` field of the result is `null`.

*Bonus task*

Can you identify nodes that belong to different communities in the first level of the hierarchy, but combine to the same community in the next level?

== Louvain: cleanup

To clean up the in-memory graph created during the Louvain exercise, run the following query:

[source,cypher]
----
CALL gds.graph.drop('got-weighted-interactions');
----


== Node Similarity

image::https://miro.medium.com/max/4000/0*ZjP7pSSaidIgSDmm.png[float="right", width="350"]

The Node Similarity algorithm compares pairs of nodes in a graph based on their connections to other nodes.
Two nodes are considered similar if they share many of the same neighbors.

The algorithm uses the so-called _Jaccard Similarity Score_ to obtain a similarity measure between two sets.
More precisely, the similarity between two nodes A and B is given by the following formula:

Similarity (A,B) = [#nodes neighboring A and B] / [#nodes neighboring A or B (or both)]

That is, nodes A and B are similar if most nodes that are neighbors to either node are also neighbors to both.

*How it works*

The input of this algorithm is a bipartite, connected graph containing two disjoint node sets.
Each relationship starts from a node in the first node set and ends at a node in the second node set.
The Node Similarity algorithm compares all nodes from the first node set with each other based on their relationships to nodes in the second set.
The complexity of this comparison grows quadratically with the number of nodes to compare.
The algorithm reduces the complexity by ignoring disconnected nodes.

For more information, see https://neo4j.com/docs/graph-data-science/current/algorithms/node-similarity/[the documentation^].

== Node Similarity: example graph

Before you run the Node Similarity algorithm, you have to create a projected graph that consists of GOT characters and the various entities to which they relate.
The task will be to find similar characters by comparing the books they appear or die in, and the houses and cultures to which they belong.
It is a bipartite graph between `Person` on one side and `Book`, `House`, and `Culture` on the other side.

You create the graph using the following query:

[source, cypher]
----
CALL gds.graph.create('got-character-related-entities', ['Person', 'Book', 'House', 'Culture'], '*')
----

This graph creation uses projection with multiple node labels.
You load all types of relationships with `*`.

== Node Similarity: simple run

Now, you can run Node Similarity with the default settings and extract the top 10 most similar pairs of characters.
The algorithm computes similarities only for `Person` nodes as they are the only nodes with outgoing edges.
To get more interesting results, you can limit the result by using the property `degreeCutoff`, to get only characters with at least 20 related entities.

[source, cypher]
----
CALL gds.nodeSimilarity.stream(
  'got-character-related-entities',
  {
    degreeCutoff: 20
  }
)
YIELD node1, node2, similarity
RETURN gds.util.asNode(node1).name AS character1, gds.util.asNode(node2).name AS character2, similarity
ORDER BY similarity DESC
LIMIT 10
----

== Node Similarity: similarity cutoff

In most real-world graphs, the number of pairs of nodes to compare is huge, and most pairs are not similar.
Therefore, it is useful to be able to limit the output.
There are several ways to deal with this.
One way is to set a threshold for a minimum similarity by specifying the `similarityCutoff` property.

[source, cypher]
----
CALL gds.nodeSimilarity.stream(
  'got-character-related-entities',
  {
    degreeCutoff: 20,
    similarityCutoff: 0.45
  }
)
YIELD node1, node2, similarity
RETURN gds.util.asNode(node1).name AS character1, gds.util.asNode(node2).name AS character2, similarity
ORDER BY similarity DESC
----

Note that you no longer need to use the LIMIT clause.

By default, the `similarityCutoff` value is a very small number, effectively filtering out pairs that have zero similarity.

== Node Similarity: topN

You can also limit the number of similarities returned by using the `topN` config option.

[source, cypher]
----
CALL gds.nodeSimilarity.stream(
  'got-character-related-entities',
  {
    degreeCutoff: 20,
    topN: 10
  }
)
YIELD node1, node2, similarity
RETURN gds.util.asNode(node1).name AS character1, gds.util.asNode(node2).name AS character2, similarity
ORDER BY similarity DESC
----

This algorithm specific way of limiting is more memory efficient than constructing the entire stream and using the LIMIT clause afterwards.

== Node Similarity: topK

Another way to limit the results is the `topK` config option.
The algorithm output will be the `K` most similar characters for each character.
Let's set this value to 1, and see if Loras Tyrell has only one similar neighbor instead of two.

[source, cypher]
----
CALL gds.nodeSimilarity.stream(
  'got-character-related-entities',
  {
    degreeCutoff: 20,
    topN: 10,
    topK: 1
  }
)
YIELD node1, node2, similarity
RETURN gds.util.asNode(node1).name AS character1, gds.util.asNode(node2).name AS character2, similarity
ORDER BY similarity DESC
----

Did you notice anything surprising?
Loras Tyrell still appeared twice as character2. +
The algorithm returns only the most similar character to Loras when considering his neighbors.
The explanation is that when considering other characters, multiple ones may have Loras as their most similar neighbor.

== Node Similarity: bottomN and bottomK

Similarly to the `topN` and `topK`, `bottomN` and `bottomK` config options limit the results but return the least similar pairs.

Why don't you try it yourself?

== Node Similarity: writing

Now, let's see how to write similarity scores back to Neo4j.
The output of the algorithm can be written as weighted relationships.
The weight property is set to the computed node similarity of the relationship it concerns.
The config option `writeProperty` specifies the name of the property.

[source, cypher]
----
CALL gds.nodeSimilarity.write(
  'got-character-related-entities',
  {
    degreeCutoff: 20,
    topN: 10,
    topK: 1,
    writeRelationshipType: 'SIMILARITY',
    writeProperty: 'character_similarity'
  }
)
----

The result is 10 relationships caused by the `topN` value.

== Node Similarity: cleanup

To clean up the in-memory graph created during the tutorial, you can run the following query:

[source, cypher]
----
CALL gds.graph.drop('got-character-related-entities');
----


== Triangle Count

*Since GDS 1.2*

A triangle in a graph is a set of three nodes all connected to each other.
The triangle count of a node is the number of triangles that node belongs to.
The Graph Data Science library provides procedures for all standard execution modes in the namespace `gds.triangleCount`.
The algorithm is only defined for undirected graphs, so we make sure to fulfil this requirement in the examples below.

== Triangle Count: examples

In order to better understand the concept of triangle counting, let us visualize a part of the GoT graph.
We will select two characters and only include relationships from the first book between them and their neighbours.
First make sure to uncheck 'Connect result nodes' in the settings of Neo4j Browser.

[source, cypher]
----
MATCH (n:Person)-[r:INTERACTS_1]->(m:Person)
WHERE n.name IN ["Robb Stark", "Tyrion Lannister"]
RETURN n, m, r
----

How many triangles do you see?

== Triangle Count: examples

Let us verify with the triangle count procedure executed on the same subgraph as above.

[source, cypher]
----
CALL gds.graph.create.cypher('small_got',
'MATCH (n:Person) RETURN id(n) AS id',
"MATCH (n:Person)-[r:INTERACTS_1]->(m:Person)
  WHERE n.name IN ['Robb Stark', 'Tyrion Lannister'] RETURN id(n) AS source, id(m) AS target
  UNION MATCH (n:Person)-[r:INTERACTS_1]->(m:Person) WHERE n.name IN ['Robb Stark', 'Tyrion Lannister'] RETURN id(m) AS source, id(n) AS target")
----

[source, cypher]
----
CALL gds.triangleCount.stream('small_got')
YIELD nodeId, triangleCount
WITH gds.util.asNode(nodeId).name AS name, triangleCount
WHERE triangleCount > 0
RETURN name, triangleCount
----

As you might have seen, indeed there are exactly two triangles which give Tyrion and Robb triangle counts of two and Tywin and Yoren triangle counts of one.

== Triangle Count: examples

For finding the people with the highest overall triangle count in book 1, we can do the following:

.This will create the named graph we are going to use in the examples (run if not already created)
[source, cypher]
----
CALL gds.graph.create(
  'got-interactions-1',
  'Person',
  {
    INTERACTS_1: {
      orientation: 'UNDIRECTED'
    }
  }
);
----

[source, cypher]
----
CALL gds.triangleCount.stream('got-interactions-1')
YIELD nodeId, triangleCount
RETURN gds.util.asNode(nodeId).name AS name, triangleCount
ORDER BY triangleCount DESC
LIMIT 10
----

Does perhaps Eddard Stark have an inclination to triangle dramas?

== Triangle Count: stats mode

The stats mode can be used to compute the total number of triangles in the graph.

[source, cypher]
----
CALL gds.triangleCount.stats('got-interactions-1')
YIELD globalTriangleCount
----

== Triangle Count: max degree

For nodes with a high degree, it is expensive to compute the triangle count.
One can exclude certain nodes from the computation by setting the configuration option `maxDegree` as follows.
For each excluded node, the triangle count will be reported as `-1`.
These nodes will also be excluded from the triangle counts of the adjacent nodes.

[source, cypher]
----
CALL gds.triangleCount.stream('got-interactions-1', {maxDegree: 10})
YIELD nodeId, triangleCount
WHERE triangleCount <> 0
RETURN gds.util.asNode(nodeId).name AS name, triangleCount
LIMIT 20
----

We note that for example Eddard Stark is no longer in the top list of high triangle count characters since his degree 51 exceeds the `maxDegree` setting.
Moreover, the triangle counts for nodes of lower degrees are also affected.
You can verify this by running the query below with and without `maxDegree`.

[source, cypher]

----
CALL gds.triangleCount.stream('got-interactions-1', {maxDegree: 10})
YIELD nodeId, triangleCount
WITH gds.util.asNode(nodeId).name AS name, triangleCount
WHERE name = "Halder"
RETURN name, triangleCount
----

== Triangle Count: cleanup

To clean up the in-memory graph created during the Triangle Count exercise, run the following query:

[source,cypher]
----
CALL gds.graph.drop('got-interactions-1');
----


== Local Clustering Coefficient

*Since GDS 1.2*

The local clustering coefficient is a metric quantifying how connected the neighborhood of a node is.
It is the probability that two random neighbors of the node are connected in the graph.
This can be obtained from the triangle count and the degree of the node.
The Graph Data Science library provides procedures for all standard execution modes in the namespace `gds.localClusteringCoefficient`.
The algorithm is only defined for undirected graphs, so we make sure to fulfil this requirement in the examples below.

== Local Clustering Coefficient: stream mode

For finding the people with the highest overall local clustering coefficient in book 1, we can do the following:

.This will create the named graph we are going to use in the examples (run if not already created)
[source, cypher]
----
CALL gds.graph.create(
  'got-interactions-1',
  'Person',
  {
    INTERACTS_1: {
      orientation: 'UNDIRECTED'
    }
  }
);
----

[source, cypher]
----
CALL gds.localClusteringCoefficient.stream('got-interactions-1')
YIELD nodeId, localClusteringCoefficient
RETURN gds.util.asNode(nodeId).name AS name, localClusteringCoefficient
ORDER BY localClusteringCoefficient DESC
LIMIT 10
----

We see here multiple nodes with local clustering coefficient 1.0, however they have in fact only few neighbors and triangles, sometimes a single triangle.
In the following example we will identify nodes with high local clustering coefficient but filter out nodes with low triangle count.

== Local Clustering Coefficient: triangleCountProperty

To compute the local clustering coefficient we need to know the number of triangles for each node.
The Local Clustering Coefficient is capable of reusing previously computed triangle counts.

First we compute the triangle counts and save them in the in-memory graph as a node property called `triangleCount`.

[source, cypher]
----
CALL gds.triangleCount.mutate('got-interactions-1', {mutateProperty: "triangleCount"})
----

In the following, we look at nodes which have both a high triangle count and a high local clustering coefficient.

[source, cypher]
----
CALL gds.localClusteringCoefficient.stream('got-interactions-1', {triangleCountProperty: "triangleCount"})
YIELD nodeId, localClusteringCoefficient AS lcc
WITH gds.util.asNode(nodeId).name AS name , lcc, gds.util.nodeProperty('got-interactions-1', nodeId, "triangleCount") AS triangleCount
WHERE triangleCount > 50
RETURN name, lcc, triangleCount
ORDER BY lcc DESC
LIMIT 10
----

The persons we see here might be regarded as central in medium to large communities.

== Local Clustering Coefficient: stats mode

To see if the GoT person graph of book 1 is a small-world network, we can run the following:

[source, cypher]
----
CALL gds.localClusteringCoefficient.stats('got-interactions-1')
YIELD averageClusteringCoefficient
----

As we see, the average clustering coefficient of around 0.036 is rather small.
In comparison, clustering coefficients of 0.11 have been reported for the world wide web and 0.59 for a network of company directors.
The explanation for the lack of small world structure could be that there are many characters in GoT, and it would require even more pages to turn them into a small-world network.

== Local Clustering Coefficient: cleanup

To clean up the in-memory graph created during the Local Clustering Coefficient exercise, run the following query:

[source,cypher]
----
CALL gds.graph.drop('got-interactions-1');
----


== Betweenness Centrality

image::https://upload.wikimedia.org/wikipedia/commons/6/60/Graph_betweenness.svg[float="right", width="300"]

*Since GDS 1.3*

Betweenness Centrality is a way of detecting the amount of influence a node has over the flow of information in a graph.
It is often used to find nodes that serve as a bridge from one part of a graph to another.

*How Betweenness Centrality works*

The algorithm calculates unweighted shortest paths between all pairs of nodes in a graph.
Each node receives a score, based on the number of shortest paths that pass through the node.
Nodes that more frequently lie on shortest paths between other nodes will have higher betweenness centrality scores.

== Betweenness Centrality: stream mode

Let's find out who is influential in the graph by running Betweenness Centrality.
If you have removed it from the catalog, you have to create it again:

[source, cypher]
----
CALL gds.graph.create('got-interactions', 'Person', {
  INTERACTS: {
    orientation: 'UNDIRECTED'
  }
})
----

First, you run the Betweenness Centrality algorithm in `stream` mode.

[source, cypher]
----
CALL gds.betweenness.stream('got-interactions') YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC LIMIT 10
----

If you ran Page Rank previously, you may notice that the result is similar.
You can run the Page Rank query again and compare the result.

[source, cypher]
----
CALL gds.pageRank.stream('got-interactions') YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC LIMIT 10
----

The result is similar, but not identical.
In general Betweenness Centrality is a good metric to identify bottlenecks and bridges in a graph while Page Rank is used to understand the influence of a node in a network.

== Betweenness Centrality: sampling

This algorithm is very computationally expensive.
To make it possible to run on large graphs we sample.
Sampling means we compute the shortest paths for some nodes but not for others.
The number of nodes sampled is configured using the `samplingSize` parameter.

Find out how many nodes are in your graph:

[source, cypher]
----
CALL gds.graph.list('got-interactions') YIELD nodeCount
----

Decide how large of a sample to use.
Here we run with half the node count as `sampleSize`.
The appropriate sample size for a use case will depend on the size and shape of the graph, as well as the resources (RAM and CPU) available.

[source, cypher]
----
CALL gds.betweenness.stream('got-interactions', {samplingSize: 1083}) YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS name, score
ORDER BY score DESC LIMIT 10
----

== Betweenness Centrality: stats, write and mutate

In stats mode, Betweenness Centrality will return statistical and measurement values of the centrality score.

[source, cypher]
----
CALL gds.betweenness.stats('got-interactions')
YIELD centralityDistribution
----

The same is returned by the write and mutate modes as well, in addition to writing results back to Neo4j or mutating the in-memory graph, respectively.


== The end

You just learnt how to explore the graph structure and elements by computing centrality and similarity scores, and detecting communities. +
To learn more about the Neo4j Graph Data Science (GDS) library, see link:https://neo4j.com/docs/graph-data-science/current/[the documentation^].